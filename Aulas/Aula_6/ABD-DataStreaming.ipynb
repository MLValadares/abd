{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algoritmos para Big Data**\n",
    "\n",
    "**2022/23**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Stream Processing\n",
    "This lecture is about processing a stream of data. We will rely on the structure streaming library of Apache Spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured streaming\n",
    "A key aspect of structured streaming is to acquire/send data from a streaming data producer/consumer. That is, from a streaming source/sink.\n",
    "\n",
    "Apache Spark provides methods to read/write from/to a stream, \n",
    "accordingly to some formats we may select from. Of course, some kind of configuration is required.\n",
    "\n",
    "Firstly, there are the usual file-based formats like json, parquet, csv, text, and so on.\n",
    "Also, we can use socket connections to get/send text data from/to TCP servers, and more importantly, we can rely on functionalities of advanced message systems like Apache Kafka, which will play a sort of buffering role. \n",
    "\n",
    "Secondly, we have to set an output mode, which defines how the results will be delivered. For instance, to see all data every time, only updates, or just the new records.\n",
    "\n",
    "Further details can be found in https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Problem formulation\n",
    "\n",
    "This exercise builds upon the credit card fraud detection prediction notebook we have worked upon in a previous lecture about classification. Recall that the main goal at the time was to create a ML binary classification model to detect frauds based on the dataset available in https://www.kaggle.com/datasets/ealtman2019/credit-card-transactions . The trained and validated SVM model was saved for further use, as well as the data used for the purpose. \n",
    "\n",
    "This time around we will use the ML model that has been created but now we will deal with a stream of transactions that are expected to be processed, like it would be in in a real-time scenario. Hence, we will simulate such scenario, mostly relying on Spark's Structured Streaming.\n",
    "\n",
    "The functional requirements for the Spark program we are going to create are as follows:\n",
    "1. To load a ML model previously built.\n",
    "2. To process credit card transactions held in a simulated data stream, by applying the ML model.\n",
    "3. To explore the results obtained.\n",
    "\n",
    "\n",
    "Also, in order to speed up some processing, we will use some files that were computed in advance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we need to install some packages, e.g. matplotlib\n",
    "\n",
    "# ! pip3 install matplotlib\n",
    "# ! pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:08.158170Z",
     "start_time": "2021-03-07T19:11:07.859222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Some imports \n",
    "\n",
    "import os \n",
    "import time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Useful visualization functions\n",
    "\n",
    "Some functions that we can use to plot data but as Python dataframes.\n",
    "\n",
    "**Disclaimer**: these functions are broadly distributed among users. Further adjustments are needed and/or advisable. Feel free to use your own plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBar(df, xcol, ycol, huecol=None):\n",
    "    sns.barplot(data=df, x=xcol, y=ycol, hue=huecol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCorrelationMatrix(df, annot=False):\n",
    "    # compute the correlation matrix\n",
    "    corr = df.corr()\n",
    "    \n",
    "    # generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    # set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    #cmap='coolwarm'\n",
    "\n",
    "    # draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=annot,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:08.961000Z",
     "start_time": "2021-03-07T19:11:08.954809Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Some Spark related imports we will use hereafter\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:12.480883Z",
     "start_time": "2021-03-07T19:11:12.479044Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 17:17:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Build a SparkSession instance if one does not exist. Notice that we can only have one per JVM\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Streaming\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Data collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:08.953150Z",
     "start_time": "2021-03-07T19:11:08.185863Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ! pwd & ls -la\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recalling from the previous classification notebook ...\n",
    "\n",
    "Regarding data:\n",
    "\n",
    "df_cards.write.mode(\"overwrite\").parquet(\"cards\")\n",
    "df_users.write.mode(\"overwrite\").parquet(\"users\")\n",
    "df_transactions.write.mode(\"overwrite\").parquet(\"credit_card_transactions\")\n",
    "small_df_transactions.write.mode(\"overwrite\").parquet(\"small-credit_card_transactions\")\n",
    "\n",
    "As for the model:\n",
    "\n",
    "pipeline = Pipeline(stages=[string_indexer, ohe_encoder, vec_assembler, lsvc])\n",
    "pipeline_model = pipeline.fit(df_train)\n",
    "df_prediction = pipeline_model.transform(df_test)\n",
    "pipeline.save(\"pipeline-LinearSVM\")\n",
    "pipeline_model.save(\"model-LinearSVM\")\n",
    "\n",
    "Notice that we are assuming the model has been created so it is going to be used as is \n",
    "(we did not save the train/test datasets anyway)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data Stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have no real time scenario in place, we will simulate a data stream by creating a built-in `rate` source to generate events at 1-second intervals, and join those 'ticks' with data from our downloaded dataset. This results in a regular stream of sample values. \n",
    "\n",
    "Alternatively, for pratical applications, we could have used an Apache Kafka source or even a file source.\n",
    "Apache Kafka was the best solution for that matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_source = spark.readStream.format(\"rate\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rate_source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:12.476668Z",
     "start_time": "2021-03-07T19:11:08.962435Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "df_transactions = spark.read.parquet(\"credit_card_transactions\")\n",
    "#df_transactions = spark.read.parquet(\"small_credit_card_transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User: integer (nullable = true)\n",
      " |-- Card: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Use Chip: string (nullable = true)\n",
      " |-- Merchant Name: long (nullable = true)\n",
      " |-- Merchant City: string (nullable = true)\n",
      " |-- Merchant State: string (nullable = true)\n",
      " |-- Zip: double (nullable = true)\n",
      " |-- MCC: integer (nullable = true)\n",
      " |-- Is Fraud?: string (nullable = true)\n",
      " |-- Correct Amount: float (nullable = true)\n",
      "\n",
      "-RECORD 0-----------------------------\n",
      " User           | 0                   \n",
      " Card           | 0                   \n",
      " Year           | 2002                \n",
      " Month          | 9                   \n",
      " Day            | 1                   \n",
      " Time           | 2023-05-02 06:21:00 \n",
      " Use Chip       | Swipe Transaction   \n",
      " Merchant Name  | 3527213246127876953 \n",
      " Merchant City  | La Verne            \n",
      " Merchant State | CA                  \n",
      " Zip            | 91750.0             \n",
      " MCC            | 5300                \n",
      " Is Fraud?      | No                  \n",
      " Correct Amount | 134.09              \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24386900"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data\n",
    "\n",
    "df_transactions.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# We may use data sorted by time\n",
    "\n",
    "# df_transactions = df_transactions.sort('Time', ascending=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Just to remember ... number of transactions by year-month-day\n",
    "\n",
    "df_plot = ( df_transactions\n",
    "            .groupby(['Year', 'Month', 'Day'])\n",
    "            .count()\n",
    "            .withColumn('Year-Month-Day', F.concat('Year',F.lit('_'),'Month',F.lit('_'),'Day'))\n",
    "            .sort('Year-Month-Day', ascending=True)\n",
    "            .toPandas()\n",
    "          )\n",
    "plotBar(df_plot, 'Year-Month-Day', 'count')\n",
    "plt.title('Number of transactions by year-month-day')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_plot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a continuous data stream\n",
    "Circularly replaying the data as long as the process is running. It will be a simulated streaming version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we have added two derived columns while creating the model, but only after storing data, \n",
    "# let us add them here\n",
    "\n",
    "df_transactions = ( df_transactions\n",
    "                .withColumn(\"Hour\", F.hour(F.col('Time')))\n",
    "                .withColumn(\"Min\", F.minute(F.col('Time')))          \n",
    "           )\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Useful function to add a column with a sequential ID\n",
    "\n",
    "def dfZipWithIndex (df, col_name=\"RowID\"):\n",
    "    new_schema = df.withColumn(col_name, F.lit(1)).schema\n",
    "    zipped_rdd = df.rdd.zipWithIndex().map(lambda row,row_id: ([row_id+1] + list(row)))\n",
    "    return spark.createDataFrame(zipped_rdd, new_schema)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add an id to data transactions ... it takes too much time\n",
    "\n",
    "df_transactions = dfZipWithIndex(df_transactions, 'Transaction_Id')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# As monotonically_increasing_id() does not work with data stream, a work-around could be:\n",
    "\n",
    "( df_transactions\n",
    "     .withColumn('Transaction_Id', F.monotonically_increasing_id())\n",
    "     .write.mode(\"overwrite\")\n",
    "     .parquet(\"credit_card_transactions_stream\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up things, let us read a data stream computed in advance (see codes above)\n",
    "\n",
    "df_transactions = spark.read.parquet(\"credit_card_transactions_stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User: integer (nullable = true)\n",
      " |-- Card: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Use Chip: string (nullable = true)\n",
      " |-- Merchant Name: long (nullable = true)\n",
      " |-- Merchant City: string (nullable = true)\n",
      " |-- Merchant State: string (nullable = true)\n",
      " |-- Zip: double (nullable = true)\n",
      " |-- MCC: integer (nullable = true)\n",
      " |-- Is Fraud?: string (nullable = true)\n",
      " |-- Correct Amount: float (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Min: integer (nullable = true)\n",
      " |-- Transaction_Id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " User           | 0                   \n",
      " Card           | 0                   \n",
      " Year           | 2002                \n",
      " Month          | 9                   \n",
      " Day            | 1                   \n",
      " Time           | 2023-05-02 06:21:00 \n",
      " Use Chip       | Swipe Transaction   \n",
      " Merchant Name  | 3527213246127876953 \n",
      " Merchant City  | La Verne            \n",
      " Merchant State | CA                  \n",
      " Zip            | 91750.0             \n",
      " MCC            | 5300                \n",
      " Is Fraud?      | No                  \n",
      " Correct Amount | 134.09              \n",
      " Hour           | 6                   \n",
      " Min            | 21                  \n",
      " Transaction_Id | 0                   \n",
      "-RECORD 1-----------------------------\n",
      " User           | 0                   \n",
      " Card           | 0                   \n",
      " Year           | 2002                \n",
      " Month          | 9                   \n",
      " Day            | 1                   \n",
      " Time           | 2023-05-02 06:42:00 \n",
      " Use Chip       | Swipe Transaction   \n",
      " Merchant Name  | -727612092139916043 \n",
      " Merchant City  | Monterey Park       \n",
      " Merchant State | CA                  \n",
      " Zip            | 91754.0             \n",
      " MCC            | 5411                \n",
      " Is Fraud?      | No                  \n",
      " Correct Amount | 38.48               \n",
      " Hour           | 6                   \n",
      " Min            | 42                  \n",
      " Transaction_Id | 1                   \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circularly replay the data\n",
    "\n",
    "data_stream = ( rate_source\n",
    "                   .select(F.expr(f'value % {transactions_count}').alias('Transaction_Id'), 'timestamp')\n",
    "                   .join(df_transactions, 'Transaction_Id')\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction_Id: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- User: integer (nullable = true)\n",
      " |-- Card: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Use Chip: string (nullable = true)\n",
      " |-- Merchant Name: long (nullable = true)\n",
      " |-- Merchant City: string (nullable = true)\n",
      " |-- Merchant State: string (nullable = true)\n",
      " |-- Zip: double (nullable = true)\n",
      " |-- MCC: integer (nullable = true)\n",
      " |-- Is Fraud?: string (nullable = true)\n",
      " |-- Correct Amount: float (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Min: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = data_stream.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the binary classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the ML model via pipeline api (not the simple pipeline)\n",
    "\n",
    "persisted_model = PipelineModel.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexerModel: uid=StringIndexer_e1064637f5a0, handleInvalid=skip, numInputCols=2, numOutputCols=2,\n",
       " OneHotEncoderModel: uid=OneHotEncoder_6a398697f374, dropLast=true, handleInvalid=error, numInputCols=2, numOutputCols=2,\n",
       " VectorAssembler_f41b5139a541,\n",
       " LinearSVCModel: uid=LinearSVC_912312635f03, numClasses=2, numFeatures=11838]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model\n",
    "\n",
    "persisted_model.stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming data transformer\n",
    "\n",
    "Let us set the operation to be applied to the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML model directly applied to the streaming dataframe using `transform`\n",
    "\n",
    "prediction_stream = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction_Id: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- User: integer (nullable = true)\n",
      " |-- Card: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Use Chip: string (nullable = true)\n",
      " |-- Merchant Name: long (nullable = true)\n",
      " |-- Merchant City: string (nullable = true)\n",
      " |-- Merchant State: string (nullable = true)\n",
      " |-- Zip: double (nullable = true)\n",
      " |-- MCC: integer (nullable = true)\n",
      " |-- Is Fraud?: string (nullable = true)\n",
      " |-- Correct Amount: float (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Min: integer (nullable = true)\n",
      " |-- Use Chip Index: double (nullable = false)\n",
      " |-- Merchant City Index: double (nullable = false)\n",
      " |-- Use Chip OHE: vector (nullable = true)\n",
      " |-- Merchant City OHE: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consuming predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to do something with the prediction data.\n",
    "For the time being, we are going to limit this step to just querying the data. \n",
    "\n",
    "For real-world application, we can offer this kind of service to other applications.\n",
    "\n",
    "Maybe in the form of an HTTP-based API or through pub/sub messaging interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transaction_Id',\n",
       " 'timestamp',\n",
       " 'User',\n",
       " 'Card',\n",
       " 'Year',\n",
       " 'Month',\n",
       " 'Day',\n",
       " 'Time',\n",
       " 'Use Chip',\n",
       " 'Merchant Name',\n",
       " 'Merchant City',\n",
       " 'Merchant State',\n",
       " 'Zip',\n",
       " 'MCC',\n",
       " 'Is Fraud?',\n",
       " 'Correct Amount',\n",
       " 'Hour',\n",
       " 'Min',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_check.append('prediction')\n",
    "cols_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case we want to start a table containing results but from scratch\n",
    "\n",
    "spark.sql(\"drop table if exists cardtransactionstable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we want to store in an in-memory table (the sink). \n",
    "# The query name will be the table name\n",
    "\n",
    "# After executing the code, the streaming computation will start in the background\n",
    "\n",
    "query_1 = ( prediction_stream\n",
    "                        .select(cols_to_check)\n",
    "                        .writeStream\n",
    "                        .queryName(\"cardtransactionstable\")\n",
    "                        .outputMode(\"append\")  # append, update\n",
    "                        .format(\"memory\")\n",
    "                        .start()\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup an aggregation by day concerning the number of frauds detected\n",
    "#\n",
    "# We leave this as exercise\n",
    "\n",
    "# fraud_count = ...\n",
    "\n",
    "# query_2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some extra checks\n",
    "\n",
    "spark.streams.active[0].isActive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_1.status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1.lastProgress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the tables we hold\n",
    "\n",
    "spark.sql(\"show tables\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactively query in-memory table\n",
    "\n",
    "spark.sql(\"select * from cardtransactionstable\").show(vertical=True, truncate=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:>                 (0 + 8) / 9][Stage 60:>                 (0 + 0) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      89|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 17:19:25 WARN DAGScheduler: Broadcasting large task binary with size 1498.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:>                                                         (0 + 8) / 9]\r"
     ]
    }
   ],
   "source": [
    "# Interactively another query in-memory table\n",
    "\n",
    "spark.sql(\"select count(*) from cardtransactionstable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactively another query in-memory table\n",
    "\n",
    "# spark.sql ..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Visual analysis ... we leave it as an exercise!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can turn off the query now and eventually set up a different one\n",
    "\n",
    "query_1.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that in a production environment, we have to establish \n",
    "# that the query is awaiting termination so to prevent the driver \n",
    "# process from termination when the stream is ative\n",
    "\n",
    "# query_1.awaitTermination()\n",
    "\n",
    "# query_2.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Additional exercise\n",
    "\n",
    "Once this exercise is completed, create a new notebook with similar implementation but using a different streaming setup. Specifically, also relying on the messaging system Apache Kafka.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Learning Spark - Lightning-Fast Data Analytics, 2nd Ed. J. Damji, B. Wenig, T. Das, and D. Lee. O'Reilly, 2020\n",
    "* Stream Processing with Apache Spark. G. Maas and F. Garillot. O'Reilly, 2019\n",
    "* Spark: The Definitive Guide - Big Data Processing Made Simple, 1st Ed. B. Chambers and M. Zaharia. O'Reilly, 2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "249px",
    "width": "332px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.98px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

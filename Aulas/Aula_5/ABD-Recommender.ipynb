{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algoritmos para Big Data**\n",
    "\n",
    "**2022/23**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Recommender Systems\n",
    "This lecture is about recommender systems (or recommendation systems). In the meantime, we highlight the usefulness of Spark SQL, particularly when it relates to persistent tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Spark SQL\n",
    "\n",
    "As mentioned in the initial lectures, Spark SQL is a Spark module for structured data processing. It works alongside the APIs of DataFrame and Dataset and it is responsible for performing extra optimizations. We can also execute SQL queries and reading data from various files formats an Hive tables. (Apache Hive can manage large datasets residing in distributed storage using SQL)\n",
    "\n",
    "Further details can be found in https://spark.apache.org/docs/latest/sql-programming-guide.html  and https://spark.apache.org/docs/latest/api/sql/index.html\n",
    "\n",
    "We can check the reference guide for Structured Query Language (SQL) which includes syntax, semantics, keywords, and examples for common SQL usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Problem formulation\n",
    "\n",
    "This exercise aims to build a recommender system of books, with focus on the recommendation model\n",
    "itself.\n",
    "The functional requirements for the Spark program we want to create are as follows:\n",
    "1. To load the dataset and perform Exploratory Data Analysis (EDA), then store the information properly cleaned, including as SQL tables.\n",
    "2. To create a recommendation model supported by the ALS algorithm provided by Spark MLlib.\n",
    "3. To pre-compute recommendations and store them in SQL tables.\n",
    "4. To show recommendations.\n",
    "\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "\n",
    "The data we are processing is from the dataset **Book-Crossing**. As stated in the website from where it can be downloaded, http://www2.informatik.uni-freiburg.de/~cziegler/BX/ , the BookCrossing (BX) dataset was collected by Cai-Nicolas Ziegler in a 4-week crawl (August / September 2004) from the Book-Crossing community with kind permission from Ron Hornbaker, CTO of Humankind Systems. It contains 278,858 users (anonymized but with demographic information) providing 1,149,780 ratings (explicit / implicit) about 271,379 books.\n",
    "\n",
    "Alternatively, we can use the command *wget* from the Terminal to download the dataset:\n",
    "\n",
    "    wget http://www2.informatik.uni-freiburg.de/~cziegler/BX/BX-CSV-Dump.zip\n",
    "\n",
    "\n",
    "The dataset comprises 3 tables, as follows:\n",
    "- **BX-Users**. Contains the users. Note that user IDs ( User-ID ) have been anonymized and map to integers. Demographic data is provided ( Location , Age ) if available. Otherwise, these fields contain NULL-values.\n",
    "- **BX-Books**. Books are identified by their respective ISBN. Invalid ISBNs have already been removed from the dataset. Moreover, some content-based information is given ( Book-Title , Book-Author , Year-Of-Publication , Publisher ), obtained from Amazon Web Services. Note that in case of several authors, only the first is provided. URLs linking to cover images are also given, appearing in three different flavours ( Image-URL-S , Image-URL-M , Image-URL-L ), i.e., small, medium, large. These URLs point to the Amazon web site.\n",
    "- **BX-Book-Ratings**. Contains the book rating information. Ratings ( Book-Rating ) are either explicit, expressed on a scale from 1-10 (higher values denoting higher appreciation), or implicit, expressed by 0.\n",
    "The columns are separated by ; and all files contain the correspondent header.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we need to install some packages, e.g. matplotlib\n",
    "\n",
    "# ! pip3 install matplotlib\n",
    "# ! pip3 install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional packages and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:08.158170Z",
     "start_time": "2021-03-07T19:11:07.859222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Some imports \n",
    "\n",
    "import os \n",
    "import sys\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Useful visualization functions\n",
    "\n",
    "Some functions that we can use to plot data but as Python dataframes.\n",
    "\n",
    "**Disclaimer**: these functions are broadly distributed among users. Further adjustments are needed and/or advisable. Feel free to use your own plotting functions.\n",
    "\n",
    "Remember: *\"A picture is worth a thousand words\"*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot(df, xcol, ycol):\n",
    "    sns.lineplot(data=df, x=xcol, y=ycol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBar(df, xcol, ycol, huecol=None):\n",
    "    sns.barplot(data=df, x=xcol, y=ycol, hue=huecol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistogram(df, xcol, huecol=None):\n",
    "    sns.histplot(data=df, x=xcol, hue=huecol, multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plotScatter(df, xcol, ycol, huecol):\n",
    "    sns.set_theme(style=\"white\")\n",
    "    sns.scatterplot(data=df, x=xcol, y=ycol, hue=huecol)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plotScatterMatrix(df, huecol):\n",
    "    sns.pairplot(data=df, hue=huecol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCorrelationMatrix(corr, annot=False):\n",
    "    # generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    # set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    #cmap='coolwarm'\n",
    "\n",
    "    # draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=annot,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Collect and label data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Checking working diretory and data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:08.953150Z",
     "start_time": "2021-03-07T19:11:08.185863Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "! pwd \n",
    "! ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"User-ID\";\"Location\";\"Age\"\n",
      "\"1\";\"nyc, new york, usa\";NULL\n",
      "\"278857\";\"knoxville, tennessee, usa\";NULL\n",
      "\"278858\";\"dublin, n/a, ireland\";NULL\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 BX-Users.csv\n",
    "! tail -n 2 BX-Users.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ISBN\";\"Book-Title\";\"Book-Author\";\"Year-Of-Publication\";\"Publisher\";\"Image-URL-S\";\"Image-URL-M\";\"Image-URL-L\"\n",
      "\"0195153448\";\"Classical Mythology\";\"Mark P. O. Morford\";\"2002\";\"Oxford University Press\";\"http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg\"\n",
      "\"0192126040\";\"Republic (World's Classics)\";\"Plato\";\"1996\";\"Oxford University Press\";\"http://images.amazon.com/images/P/0192126040.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0192126040.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0192126040.01.LZZZZZZZ.jpg\"\n",
      "\"0767409752\";\"A Guided Tour of Rene Descartes' Meditations on First Philosophy with Complete Translations of the Meditations by Ronald Rubin\";\"Christopher  Biffle\";\"2000\";\"McGraw-Hill Humanities/Social Sciences/Languages\";\"http://images.amazon.com/images/P/0767409752.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0767409752.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0767409752.01.LZZZZZZZ.jpg\"\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 BX-Books.csv\n",
    "! tail -n 2 BX-Books.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"User-ID\";\"ISBN\";\"Book-Rating\"\n",
      "\"276725\";\"034545104X\";\"0\"\n",
      "\"276721\";\"0590442449\";\"10\"\n",
      "\"276723\";\"05162443314\";\"8\"\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 BX-Book-Ratings.csv\n",
    "! tail -n 2 BX-Book-Ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:08.961000Z",
     "start_time": "2021-03-07T19:11:08.954809Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# some Spark related imports we will use hereafter\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:12.480883Z",
     "start_time": "2021-03-07T19:11:12.479044Z"
    },
    "hidden": true
   },
   "source": [
    "# Build a SparkSession instance if one does not exist. Notice that we can only have one per JVM\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Recommender\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\",6)\\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:12.476668Z",
     "start_time": "2021-03-07T19:11:08.962435Z"
    },
    "hidden": true
   },
   "source": [
    "## Reading datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = spark.read.csv(\"BX-Users.csv\", header=\"true\", \n",
    "                              inferSchema=\"true\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking data\n",
    "Schema, show and count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User-ID: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n",
      "+-------+----------------------------------+----+\n",
      "|User-ID|Location                          |Age |\n",
      "+-------+----------------------------------+----+\n",
      "|1      |nyc, new york, usa                |NULL|\n",
      "|2      |stockton, california, usa         |18  |\n",
      "|3      |moscow, yukon territory, russia   |NULL|\n",
      "|4      |porto, v.n.gaia, portugal         |17  |\n",
      "|5      |farnborough, hants, united kingdom|NULL|\n",
      "+-------+----------------------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "278859"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.printSchema()\n",
    "df_users.show(5, truncate=False) \n",
    "num_users = df_users.count()\n",
    "num_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ISBN: string (nullable = true)\n",
      " |-- Book-Title: string (nullable = true)\n",
      " |-- Book-Author: string (nullable = true)\n",
      " |-- Year-Of-Publication: integer (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- Image-URL-S: string (nullable = true)\n",
      " |-- Image-URL-M: string (nullable = true)\n",
      " |-- Image-URL-L: string (nullable = true)\n",
      "\n",
      "-RECORD 0---------------------------------------------------------------------------\n",
      " ISBN                | 0195153448                                                   \n",
      " Book-Title          | Classical Mythology                                          \n",
      " Book-Author         | Mark P. O. Morford                                           \n",
      " Year-Of-Publication | 2002                                                         \n",
      " Publisher           | Oxford University Press                                      \n",
      " Image-URL-S         | http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg \n",
      " Image-URL-M         | http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg \n",
      " Image-URL-L         | http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg \n",
      "-RECORD 1---------------------------------------------------------------------------\n",
      " ISBN                | 0002005018                                                   \n",
      " Book-Title          | Clara Callan                                                 \n",
      " Book-Author         | Richard Bruce Wright                                         \n",
      " Year-Of-Publication | 2001                                                         \n",
      " Publisher           | HarperFlamingo Canada                                        \n",
      " Image-URL-S         | http://images.amazon.com/images/P/0002005018.01.THUMBZZZ.jpg \n",
      " Image-URL-M         | http://images.amazon.com/images/P/0002005018.01.MZZZZZZZ.jpg \n",
      " Image-URL-L         | http://images.amazon.com/images/P/0002005018.01.LZZZZZZZ.jpg \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "271379"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User-ID: integer (nullable = true)\n",
      " |-- ISBN: string (nullable = true)\n",
      " |-- Book-Rating: integer (nullable = true)\n",
      "\n",
      "+-------+----------+-----------+\n",
      "|User-ID|ISBN      |Book-Rating|\n",
      "+-------+----------+-----------+\n",
      "|276725 |034545104X|0          |\n",
      "|276726 |0155061224|5          |\n",
      "|276727 |0446520802|0          |\n",
      "|276729 |052165615X|3          |\n",
      "|276729 |0521795028|6          |\n",
      "+-------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1149780"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate data\n",
    "\n",
    "Let us get some data insight, with some EDA based on descriptive statistics and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems:\n",
    "- In Users, User-ID is string but should be integer. (See Ratings)\n",
    "- In Users, Age is string but should be integer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = ( df_users\n",
    "                .withColumn('User-ID-new',  \n",
    "                .withColumn('Age-new',  \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User-ID: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- User-ID-new: integer (nullable = true)\n",
      " |-- Age-new: integer (nullable = true)\n",
      "\n",
      "+-------+----------------------------------+----+-----------+-------+\n",
      "|User-ID|Location                          |Age |User-ID-new|Age-new|\n",
      "+-------+----------------------------------+----+-----------+-------+\n",
      "|1      |nyc, new york, usa                |NULL|1          |null   |\n",
      "|2      |stockton, california, usa         |18  |2          |18     |\n",
      "|3      |moscow, yukon territory, russia   |NULL|3          |null   |\n",
      "|4      |porto, v.n.gaia, portugal         |17  |4          |17     |\n",
      "|5      |farnborough, hants, united kingdom|NULL|5          |null   |\n",
      "+-------+----------------------------------+----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the changes made \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify number of nulls or NaN in columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[278859, 168096]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[num_users, df_users.dropna().count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[271379, 271379]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[num_books, df_books.dropna().count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1149780, 1149780]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[num_ratings, df_ratings.dropna().count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nulls in Users:\n",
      "Column User-ID-new with 1 nulls or NaN, out of 278859 records (0.00%)\n",
      "Column Age-new with 110763 nulls or NaN, out of 278859 records (39.72%)\n"
     ]
    }
   ],
   "source": [
    "print('\\nNulls in Users:')\n",
    "cols_to_forget = ['User-ID', 'Age']\n",
    "users_cols_interest = [x for x in df_users.columns if x not in cols_to_forget]\n",
    "for cl in users_cols_interest:\n",
    "    k = df_users.select(cl).filter(F.col(cl).isNull() | F.isnan(cl)).count()\n",
    "    if k > 0:\n",
    "        print(f'Column {cl} with {k} nulls or NaN, out of {num_users} records ({k*100/num_users:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+----+-----------+-------+\n",
      "|        User-ID|Location| Age|User-ID-new|Age-new|\n",
      "+---------------+--------+----+-----------+-------+\n",
      "|, milan, italy\"|    NULL|null|       null|   null|\n",
      "+---------------+--------+----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users.filter(F.col('User-ID-new').isNull() | F.isnan(cl)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278858"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of that wrong record, and we may avoid using Age-new column\n",
    "\n",
    "df_users = df_users.dropna(subset=['User-ID-new'])\n",
    "num_users = df_users.count()\n",
    "num_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:**\n",
    "\n",
    "Recall that if we delete an observation in one table, still consistency among tables has to be preserved. \n",
    "\n",
    "We leave the checking as an exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[278858, 278858]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[num_users, df_users.dropDuplicates().count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[271379, 271379]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[num_books, df_books.dropDuplicates().count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1149780, 1149780]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[num_ratings, df_ratings.dropDuplicates().count()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness in Users:\n",
      "Column Location with 57309 distinct values, out of 278858 records (20.55%)\n",
      "Column User-ID-new with 278858 distinct values, out of 278858 records (100.00%)\n",
      "Column Age-new with 166 distinct values, out of 278858 records (0.06%)\n"
     ]
    }
   ],
   "source": [
    "print('\\nUniqueness in Users:')\n",
    "for cl in users_cols_interest:\n",
    "    k = df_users.select(cl).distinct().count()\n",
    "    print(f'Column {cl} with {k} distinct values, out of {num_users} records ({k*100/num_users:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness in Books:\n",
      "Column ISBN with 271379 distinct values, out of 271379 records (100.00%)\n",
      "Column Book-Title with 242154 distinct values, out of 271379 records (89.23%)\n",
      "Column Book-Author with 102028 distinct values, out of 271379 records (37.60%)\n",
      "Column Year-Of-Publication with 116 distinct values, out of 271379 records (0.04%)\n",
      "Column Publisher with 16807 distinct values, out of 271379 records (6.19%)\n",
      "Column Image-URL-S with 271063 distinct values, out of 271379 records (99.88%)\n",
      "Column Image-URL-M with 271063 distinct values, out of 271379 records (99.88%)\n",
      "Column Image-URL-L with 271063 distinct values, out of 271379 records (99.88%)\n"
     ]
    }
   ],
   "source": [
    "print('\\nUniqueness in Books:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniqueness in Ratings:\n",
      "Column User-ID with 105283 distinct values, out of 1149780 records (9.16%)\n",
      "Column ISBN with 340556 distinct values, out of 1149780 records (29.62%)\n",
      "Column Book-Rating with 11 distinct values, out of 1149780 records (0.00%)\n"
     ]
    }
   ],
   "source": [
    "print('\\nUniqueness in Ratings:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of values for numeric columns of interest, one by one. Use of describe() or summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|      User-ID-new|\n",
      "+-------+-----------------+\n",
      "|  count|           278858|\n",
      "|   mean|         139429.5|\n",
      "| stddev|80499.51502027822|\n",
      "|    min|                1|\n",
      "|    max|           278858|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|           Age-new|\n",
      "+-------+------------------+\n",
      "|  count|            168096|\n",
      "|   mean| 34.75143370454978|\n",
      "| stddev|14.428097382455421|\n",
      "|    min|                 0|\n",
      "|    max|               244|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['User-ID-new', 'Age-new']\n",
    "for cl in cols:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|Year-Of-Publication|\n",
      "+-------+-------------------+\n",
      "|  count|             271379|\n",
      "|   mean| 1959.7560496574902|\n",
      "| stddev|  258.0113625638112|\n",
      "|    min|                  0|\n",
      "|    max|               2050|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|           User-ID|\n",
      "+-------+------------------+\n",
      "|  count|           1149780|\n",
      "|   mean|140386.39512602412|\n",
      "| stddev| 80562.27771851176|\n",
      "|    min|                 2|\n",
      "|    max|            278854|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|       Book-Rating|\n",
      "+-------+------------------+\n",
      "|  count|           1149780|\n",
      "|   mean|2.8669501991685364|\n",
      "| stddev| 3.854183859201656|\n",
      "|    min|                 0|\n",
      "|    max|                10|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['User-ID', 'Book-Rating']\n",
    "for cl in cols:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems:\n",
    "- In Users, 'Age-new' ranges from 0 to 244. \n",
    "- In Books, Year-Of-Publication ranges from 0 to 2050.\n",
    "\n",
    "Clearly, some data is wrong. We should be are of that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some visualizations to better understand the data. We leave it as an exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving clean data\n",
    "\n",
    "Saving data for further use if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['User-ID-new', 'Location']\n",
    "df_clean_users = df_users.select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ISBN', 'Book-Title', 'Book-Author', 'Publisher']\n",
    "df_clean_books = df_books.select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to duplicate variables here but ... for the sake of better understanding\n",
    "\n",
    "df_clean_ratings = df_ratings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context:\n",
    "\n",
    "For the recommendation model, ratings data is critical.\n",
    "\n",
    "As usual, we may want to have a smaller dataset just for the purpose of testing locally.\n",
    "But as mentioned before, consistency among the three tables has to be guaranteed. \n",
    "\n",
    "We can also use a smaller ratings dataset, but **keeping** the complete users and books datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574185"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from counting of ratings = 1149780\n",
    "\n",
    "fraction = 0.5 # reduce to 50%\n",
    "\n",
    "seed = 5\n",
    "with_replacement = False\n",
    "df_clean_ratings_small = df_clean_ratings.sample(withReplacement=with_replacement, \n",
    "                                                 fraction=fraction, seed=seed)\n",
    "df_clean_ratings_small.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete memory consuming variables that are no longer needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Saving users\n",
    "\n",
    "output_users = \"users.parquet\"\n",
    "df_clean_users.write.mode(\"overwrite\").parquet(output_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:37:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Saving books\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:37:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Saving ratings\n",
    "\n",
    "output_ratings = \"ratings.parquet\"\n",
    "df_clean_ratings.\n",
    "\n",
    "output_ratings = \"ratings_small.parquet\"\n",
    "df_clean_ratings_small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check in the running directory if that was accomplished\n",
    "\n",
    "! ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, save them as persistent tables into Hive metastore\n",
    "\n",
    "Notes:\n",
    "- An existing Hive deployment is not necessary to use this feature. Spark will take care of it.\n",
    "- We can create a SQL table from a DataFrame with createOrReplaceTempView command, valid for the session. (there is also the option of global temporary views, to be shared among all sessions till the Spark application terminates)\n",
    "- But with saveAsTable, there will be a pointer to the data in the Hive metastore. So persistent tables will exist even after the Spark program has restarted, as long as connection is maintained to the same metastore.\n",
    "\n",
    "See details in http://spark.apache.org/docs/latest/sql-data-sources.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persistent tables into Hive metastore\n",
    "\n",
    "df_clean_users.write.mode(\"overwrite\").saveAsTable(\"UsersTable\")\n",
    "df_clean_books.\n",
    "df_clean_ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_ratings_to_use = df_clean_ratings\n",
    "# df_ratings_to_use = df_clean_ratings_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T21:59:55.649596Z",
     "start_time": "2021-03-06T21:59:55.646188Z"
    },
    "hidden": true
   },
   "source": [
    "## Overview\n",
    "After establishing the clean data to be used, we should get an overview about what we have achieved, with some statistics and visualizations.\n",
    "\n",
    "**But** \n",
    "\n",
    "we leave it as it is now, because so far there are no significant changes. Eventually, we could check the ratings and draw some plots, as it is the critical part of the system. You can have a go in that regard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T13:53:01.240297Z",
     "start_time": "2021-03-06T13:53:01.235639Z"
    },
    "hidden": true
   },
   "source": [
    "## Features transformation\n",
    "\n",
    "As mentioned, ratings are critial here. Recall that, in the dataframe, the schema is User-ID (integer), ISBN (string) and Book-Rating (integer). ISBN poses a problem as the ML algorithm requires numbers to process. Hence, we have to convert it to numbers - we will use `StringIndexer` to do so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringerIndexer for ISBN\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"ISBN\", outputCol=\"ISBN-Index\", handleInvalid=\"keep\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns from ratings that are going to be considered in the model\n",
    "\n",
    "user_col = \"User-ID\"\n",
    "item_col = \"ISBN-Index\" \n",
    "rating_col = \"Book-Rating\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T12:46:42.492329Z",
     "start_time": "2021-03-06T12:46:42.487767Z"
    },
    "hidden": true
   },
   "source": [
    "# Select and train model\n",
    "\n",
    "In order to create the recommendation model, we will use the Alternating Least Squares (ALS) algorithm provided by Spark MLlib. See details in http://spark.apache.org/docs/latest/ml-collaborative-filtering.html , as we advise to check the main assumptions the implemented algorithm relies upon. For example, notice that:\n",
    "- it underlies a collaborative filtering strategy;\n",
    "- it aims to fill in the missing entries of a user-item association matrix, in which users and items are described by a small set of latent factors that can be used to predict missing entries. The latent factors are learned by the ALS algorithm.\n",
    "\n",
    "Again, as for data to train the model, the focus is on ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Train/test split\n",
    "\n",
    "We will use the standard split 80/20, for the reasons explained in previous lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:31.983392Z",
     "start_time": "2021-03-07T19:11:30.973303Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 919970 rows in the training set and 229810 in the test set.\n"
     ]
    }
   ],
   "source": [
    "# train/test ratings split\n",
    "\n",
    "df_train, df_test = df_clean_ratings.\n",
    "\n",
    "# caching data ... but just the training part and if we want to (check the implications)\n",
    "# df_train.cache()\n",
    "\n",
    "# print the number of rows in each part\n",
    "print(f\"There are {df_train.count()} rows in the training set and {df_test.count()} in the test set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "As we did with clean data, we may consider storing the data split into files, should we want to use it elsewhere. \n",
    "This relates to the need of guaranteeing unicity in a different environment. \n",
    "We leave it as it is now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## ALS model\n",
    "\n",
    "Using the `ALS` estimator (the algorithm) to learn from the training data and consequently to build the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:32.955606Z",
     "start_time": "2021-03-07T19:11:32.419126Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "# note that we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "        \n",
    "        \n",
    "als = ALS(maxIter=5, regParam=0.01, \n",
    "          userCol=user_col, \n",
    "          itemCol=item_col, \n",
    "          ratingCol=rating_col,\n",
    "          coldStartStrategy=\"drop\",\n",
    "          implicitPrefs=True\n",
    "         )\n",
    "\n",
    "# if the rating matrix is derived from another source of information\n",
    "# (i.e. it is inferred from other signals), we may set implicitPrefs\n",
    "# to True to get better results (see ALS reference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML pipeline configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:32.970301Z",
     "start_time": "2021-03-07T19:11:32.967223Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The pipeline holds two stages set above\n",
    "\n",
    "# As we will see below, we are going to use it just for evaluation purposes\n",
    "\n",
    "pipeline = Pipeline(stages= \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "Get the model (as transformer) by fitting the pipeline to training data. It may take time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model\n",
    "\n",
    "Let us evaluate the ALS model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "\n",
    "It is time to apply the model built to test data. Again, we will use the pipeline set above. Notice that, since the pipeline model is a transformer, we can easily apply it to test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T19:11:33.280981Z",
     "start_time": "2021-03-07T19:11:32.971571Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on test data and show values of columns of interest\n",
    "\n",
    "df_prediction = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User-ID: integer (nullable = true)\n",
      " |-- ISBN: string (nullable = true)\n",
      " |-- Book-Rating: integer (nullable = true)\n",
      " |-- ISBN-Index: double (nullable = false)\n",
      " |-- prediction: float (nullable = false)\n",
      "\n",
      "23/04/22 17:39:13 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 296:>                (0 + 0) / 8][Stage 310:>               (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:14 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 296:=> (5 + 3) / 8][Stage 310:> (1 + 5) / 10][Stage 311:> (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:15 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:17 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "23/04/22 17:39:18 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n",
      "+-------+----------+-----------+----------+------------+\n",
      "|User-ID|ISBN      |Book-Rating|ISBN-Index|prediction  |\n",
      "+-------+----------+-----------+----------+------------+\n",
      "|28177  |0375727345|10         |12.0      |0.62694347  |\n",
      "|33862  |0375727345|5          |12.0      |0.032191776 |\n",
      "|32440  |0375727345|8          |12.0      |0.21301384  |\n",
      "|22605  |0375727345|7          |12.0      |0.003980553 |\n",
      "|29424  |0375727345|6          |12.0      |0.009848104 |\n",
      "|16634  |0375727345|0          |12.0      |-0.19294487 |\n",
      "|16795  |0375727345|9          |12.0      |0.8389824   |\n",
      "|35424  |0375727345|0          |12.0      |0.02602458  |\n",
      "|29168  |0375727345|0          |12.0      |0.042082306 |\n",
      "|36606  |0375727345|0          |12.0      |0.25273326  |\n",
      "|21576  |0375727345|8          |12.0      |-0.095143445|\n",
      "|14849  |0375727345|0          |12.0      |0.0         |\n",
      "|6251   |0375727345|6          |12.0      |-0.17332862 |\n",
      "|6575   |0375727345|0          |12.0      |1.0489374   |\n",
      "|35857  |0375727345|5          |12.0      |0.43122125  |\n",
      "|12285  |0375727345|9          |12.0      |0.007032776 |\n",
      "|37979  |0375727345|9          |12.0      |-0.00791256 |\n",
      "|41841  |0375727345|8          |12.0      |0.1919349   |\n",
      "|74728  |0375727345|8          |12.0      |-0.072210416|\n",
      "|75428  |0375727345|9          |12.0      |0.021363957 |\n",
      "+-------+----------+-----------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "23/04/22 17:39:18 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 333:>                (0 + 0) / 8][Stage 347:>               (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:19 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 333:>  (0 + 0) / 8][Stage 347:> (0 + 0) / 10][Stage 348:> (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:20 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:22 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "23/04/22 17:39:23 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "174536"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking its schema and content\n",
    "\n",
    "df_prediction.printSchema()\n",
    "df_prediction.show(truncate=False)\n",
    "df_prediction.count() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:25 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 387:>                (0 + 0) / 8][Stage 401:>               (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:26 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 387:>  (0 + 0) / 8][Stage 401:> (0 + 0) / 10][Stage 402:> (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:27 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:28 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "23/04/22 17:39:29 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 423:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+----------+-------------+\n",
      "|User-ID|ISBN      |Book-Rating|ISBN-Index|prediction   |\n",
      "+-------+----------+-----------+----------+-------------+\n",
      "|8      |0399135782|0          |4077.0    |-1.6880228E-9|\n",
      "|8      |0002005018|5          |16189.0   |9.2019986E-10|\n",
      "|8      |0671870432|0          |193498.0  |5.643216E-10 |\n",
      "|8      |1881320189|7          |110296.0  |2.4213542E-10|\n",
      "|10     |1841721522|0          |3434.0    |1.0123705E-12|\n",
      "|14     |0971880107|0          |0.0       |-0.0044853445|\n",
      "|17     |0553278398|0          |1829.0    |0.0014182599 |\n",
      "|32     |038078243X|0          |19957.0   |2.4111976E-6 |\n",
      "|39     |0553582909|8          |4642.0    |0.0018880817 |\n",
      "|44     |0842342702|0          |516.0     |9.4112195E-4 |\n",
      "|67     |042511774X|0          |323.0     |4.95722E-11  |\n",
      "|99     |0451166892|3          |141.0     |0.0035735366 |\n",
      "|99     |0446677450|10         |1463.0    |7.2222704E-4 |\n",
      "|99     |0312261594|8          |5372.0    |0.0011581918 |\n",
      "|99     |0553347594|9          |41050.0   |3.0804033E-4 |\n",
      "|114    |0446612618|8          |7905.0    |0.009146694  |\n",
      "|114    |0446608653|9          |1056.0    |0.016526029  |\n",
      "|160    |9728579225|8          |67796.0   |1.6077525E-10|\n",
      "|165    |0061099325|4          |1251.0    |0.0049548014 |\n",
      "|183    |9724129411|9          |296718.0  |4.3810675E-10|\n",
      "+-------+----------+-----------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_prediction.orderBy(\"User-ID\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:30 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 424:>                (0 + 0) / 8][Stage 438:>               (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:31 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 424:>  (0 + 0) / 8][Stage 438:> (0 + 0) / 10][Stage 439:> (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:32 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:33 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "23/04/22 17:39:34 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n",
      "+-------+----------+-----------+----------+-------------+\n",
      "|User-ID|ISBN      |Book-Rating|ISBN-Index|prediction   |\n",
      "+-------+----------+-----------+----------+-------------+\n",
      "|31826  |0971880107|1          |0.0       |-0.27701813  |\n",
      "|10338  |0971880107|0          |0.0       |0.0012092944 |\n",
      "|35513  |0971880107|8          |0.0       |-8.5264915E-8|\n",
      "|35823  |0971880107|0          |0.0       |0.0          |\n",
      "|7286   |0971880107|0          |0.0       |0.33245176   |\n",
      "|27740  |0971880107|0          |0.0       |0.05949734   |\n",
      "|1435   |0971880107|5          |0.0       |0.48156643   |\n",
      "|21484  |0971880107|0          |0.0       |-0.063117385 |\n",
      "|26525  |0971880107|0          |0.0       |-0.014014728 |\n",
      "|30144  |0971880107|0          |0.0       |0.009675688  |\n",
      "|14079  |0971880107|0          |0.0       |-0.115478896 |\n",
      "|26443  |0971880107|0          |0.0       |0.008276263  |\n",
      "|4781   |0971880107|0          |0.0       |-1.8554585E-8|\n",
      "|35859  |0971880107|0          |0.0       |0.5671146    |\n",
      "|13722  |0971880107|0          |0.0       |0.010174753  |\n",
      "|1025   |0971880107|0          |0.0       |0.3754246    |\n",
      "|16999  |0971880107|0          |0.0       |0.09323458   |\n",
      "|33816  |0971880107|0          |0.0       |0.31047556   |\n",
      "|15053  |0971880107|0          |0.0       |-9.665739E-7 |\n",
      "|32329  |0971880107|6          |0.0       |-0.11149354  |\n",
      "+-------+----------+-----------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_prediction.orderBy(\"ISBN-Index\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "Let us use an evaluator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:35 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "23/04/22 17:39:35 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 461:>  (0 + 0) / 8][Stage 475:> (0 + 0) / 10][Stage 476:> (0 + 0) / 10]0]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:36 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:38 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "23/04/22 17:39:39 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 497:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:39:40 WARN DAGScheduler: Broadcasting large task binary with size 12.4 MiB\n",
      "Root-mean-square error = 4.651106603145104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the model by computing the RMSE on the test data\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                labelCol=rating_col,\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "rmse = evaluator.evaluate(df_prediction)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can save the pipeline for further use should it be required\n",
    "\n",
    "pipeline.save(\"pipeline-ALS\")\n",
    "\n",
    "# later on, it can be loaded anywhere\n",
    "\n",
    "# check the content of created directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune model\n",
    "\n",
    "We can improve the model. For example, by carrying out better data cleasing operations and take into consideration efficiency issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-computing recommendations\n",
    "\n",
    "The `ALS` algorithm provides some functions to get recommendations directly. \n",
    "\n",
    "Although we can achieve results if working with predictions after the pipeline set (see below), we will take advantage of such methods directly. \n",
    "\n",
    "We should emphasize that, as it stands, we will not be using the pipeline for this task.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+-------+----------+-----------+----------+-------------+\n",
    "|User-ID|ISBN      |Book-Rating|ISBN-Index|prediction   |\n",
    "+-------+----------+-----------+----------+-------------+\n",
    "|30261  |0385504209|0          |2.0       |0.04549066   |\n",
    "|3363   |0385504209|0          |2.0       |0.03916065   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking with training data for the sake of example\n",
    "\n",
    "df_train_indexed = indexer.fit(df_train).transform(df_train)\n",
    "model = als.fit(df_train_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all distinct users and books\n",
    "\n",
    "#user_col = \"User-ID\"\n",
    "#item_col = \"ISBN-Index\" \n",
    "#rating_col = \"Book-Rating\"\n",
    "\n",
    "users = df_train_indexed.select(als.getUserCol()).distinct()\n",
    "\n",
    "books = df_train_indexed.select(als.getItemCol()).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|User-ID|\n",
      "+-------+\n",
      "|    463|\n",
      "|    496|\n",
      "|   1238|\n",
      "|   1591|\n",
      "|   1829|\n",
      "|   2366|\n",
      "|   3175|\n",
      "|   3918|\n",
      "|   4900|\n",
      "|   5300|\n",
      "|   5803|\n",
      "|   6336|\n",
      "|   6357|\n",
      "|   6466|\n",
      "|   6654|\n",
      "|   7253|\n",
      "|   7340|\n",
      "|   7754|\n",
      "|   7982|\n",
      "|   8086|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:40:10 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 626:===================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:40:12 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "+----------+\n",
      "|ISBN-Index|\n",
      "+----------+\n",
      "|   15893.0|\n",
      "|     596.0|\n",
      "|  280485.0|\n",
      "|    7313.0|\n",
      "|    2862.0|\n",
      "|     558.0|\n",
      "|     299.0|\n",
      "|  100406.0|\n",
      "|  234747.0|\n",
      "|  270298.0|\n",
      "|  191645.0|\n",
      "|  289931.0|\n",
      "|     305.0|\n",
      "|  213226.0|\n",
      "|   28481.0|\n",
      "|   80005.0|\n",
      "|   87780.0|\n",
      "|   82979.0|\n",
      "|   25813.0|\n",
      "|   13607.0|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "books.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:40:13 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 635:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:40:14 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[92938, 298820]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[users.count(), books.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top book recommendations for users\n",
    "\n",
    "top_n_books = 2\n",
    "user_recs = model.recommendForAllUsers(top_n_books)\n",
    "\n",
    "# Generate top book recommendations for a specified set of users\n",
    "\n",
    "# subset_users = users.limit(5)\n",
    "# user_subset_recs = model.recommendForUserSubset(subset_users, top_n_books)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:40:15 WARN DAGScheduler: Broadcasting large task binary with size 11.0 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 654:=====================================================>(99 + 1) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:42:58 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "+-------+-----------------------------------------+\n",
      "|User-ID|recommendations                          |\n",
      "+-------+-----------------------------------------+\n",
      "|12     |[{13, 2.2562064E-8}, {45, 1.9955959E-8}] |\n",
      "|44     |[{55, 0.020076418}, {45, 0.018091321}]   |\n",
      "|53     |[{16, 0.014748665}, {13, 0.009437016}]   |\n",
      "|78     |[{18, 0.011642931}, {13, 0.011133369}]   |\n",
      "|81     |[{5, 0.008602302}, {39, 0.006890223}]    |\n",
      "|85     |[{1, 0.008010264}, {8, 0.0037765228}]    |\n",
      "|133    |[{1, 0.016934456}, {22, 0.008972366}]    |\n",
      "|137    |[{2, 1.7402932E-10}, {45, 1.6306474E-10}]|\n",
      "|183    |[{5, 0.03477192}, {16, 0.026116533}]     |\n",
      "|193    |[{40960, 0.0}, {40970, 0.0}]             |\n",
      "|236    |[{40960, 0.0}, {40970, 0.0}]             |\n",
      "|243    |[{8, 0.4884001}, {12, 0.43862292}]       |\n",
      "|300    |[{15, 0.014299512}, {42, 0.012803585}]   |\n",
      "|362    |[{77, 0.0053710025}, {0, 0.0047543505}]  |\n",
      "|384    |[{40960, 0.0}, {40970, 0.0}]             |\n",
      "|392    |[{0, 8.4980944E-5}, {93, 6.320027E-5}]   |\n",
      "|406    |[{1, 1.5957012E-8}, {2, 1.21170505E-8}]  |\n",
      "|460    |[{10, 0.0030535515}, {0, 0.0029589625}]  |\n",
      "|463    |[{25, 0.0032482832}, {77, 0.0030188842}] |\n",
      "|472    |[{55, 0.0011070749}, {33, 0.001098036}]  |\n",
      "+-------+-----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_recs.show(truncate=False)\n",
    "\n",
    "# user_subset_recs.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top user recommendations for each book\n",
    "\n",
    "top_n_users = 2\n",
    "book_recs = model.recommendForAllItems(top_n_users)\n",
    "\n",
    "# Generate top user recommendations for a specified set of books\n",
    "\n",
    "# subset_books = books.limit(5)\n",
    "# book_subset_recs = model.recommendForItemSubset(subset_books, top_n_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:42:59 WARN DAGScheduler: Broadcasting large task binary with size 11.0 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 683:=====================================================>(99 + 1) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 17:45:37 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "+----------+-------------------------------------------+\n",
      "|ISBN-Index|recommendations                            |\n",
      "+----------+-------------------------------------------+\n",
      "|26        |[{235282, 1.3388069}, {4017, 1.2779285}]   |\n",
      "|27        |[{214786, 1.4063966}, {7346, 1.2597617}]   |\n",
      "|28        |[{235282, 1.6009766}, {254465, 1.3653027}] |\n",
      "|31        |[{76499, 1.4286722}, {31826, 1.0966935}]   |\n",
      "|34        |[{153662, 2.2438536}, {11676, 1.3453243}]  |\n",
      "|44        |[{76499, 1.6846067}, {98391, 1.5944945}]   |\n",
      "|53        |[{55490, 1.1888067}, {98391, 1.1182915}]   |\n",
      "|65        |[{142524, 0.91407824}, {232131, 0.8715034}]|\n",
      "|76        |[{153662, 1.6012522}, {11676, 1.2615935}]  |\n",
      "|78        |[{98391, 1.6920795}, {56399, 1.529717}]    |\n",
      "|81        |[{11676, 0.8763598}, {104636, 0.8072574}]  |\n",
      "|85        |[{11676, 1.2336705}, {204864, 0.86159515}] |\n",
      "|101       |[{11676, 1.1785723}, {204864, 1.1030658}]  |\n",
      "|103       |[{98391, 1.1639751}, {56399, 1.1351087}]   |\n",
      "|108       |[{230522, 0.916898}, {60244, 0.8593741}]   |\n",
      "|115       |[{95359, 0.7740643}, {60244, 0.7203063}]   |\n",
      "|126       |[{11676, 0.8900601}, {258534, 0.7349734}]  |\n",
      "|133       |[{153662, 0.9926629}, {104636, 0.955925}]  |\n",
      "|137       |[{69078, 0.9394564}, {56959, 0.913174}]    |\n",
      "|148       |[{258534, 0.7546577}, {104636, 0.71062905}]|\n",
      "+----------+-------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "book_recs.show(truncate=False)\n",
    "\n",
    "# book_subset_recs.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing recommendations as persistent tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the recommendations as persistent tables into the Hive metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recs.write.mode(\"overwrite\").saveAsTable(\"UserRecommendationsTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_recs.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It looks like we had a problem storing array<struct<ISBN-Index:int,rating:float>>\n",
    "\n",
    "Exercise: how can we sort it out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check in the running directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring results\n",
    "1. Given a user, shows the recommended list of books.\n",
    "2. Given a book, shows the list of users who might be interested on.\n",
    "\n",
    "We are going to use Spark SQL tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user to explore\n",
    "\n",
    "user = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book to explore\n",
    "\n",
    "book = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us check the SQL tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register information about users as a SQL temporary view\n",
    "\n",
    "df_clean_users.createOrReplaceTempView(\"users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register information about books as a SQL temporary view\n",
    "\n",
    "df_clean_books.createOrReplaceTempView(\"books\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', description='Default Hive database', locationUri='file:/Users/adriano/Documents/Academia/ISCTE/Teaching/2021-22/Praticas/Notebooks/Solutions/spark-warehouse')]\n"
     ]
    }
   ],
   "source": [
    "print(spark.catalog.listDatabases())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='bookrecommendationstable', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='bookstable', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='ratingstable', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='scoretable', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='userrecommendationstable', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='userstable', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='books', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='users', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " spark.catalog.listTables(dbName=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use managed tables\n",
    "\n",
    "spark.sql(\"USE default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='ISBN', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='Book-Title', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='Book-Author', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='Publisher', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns('bookstable')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------+\n",
      "|User-ID-new|Location                          |\n",
      "+-----------+----------------------------------+\n",
      "|1          |nyc, new york, usa                |\n",
      "|2          |stockton, california, usa         |\n",
      "|3          |moscow, yukon territory, russia   |\n",
      "|4          |porto, v.n.gaia, portugal         |\n",
      "|5          |farnborough, hants, united kingdom|\n",
      "|6          |santa monica, california, usa     |\n",
      "|7          |washington, dc, usa               |\n",
      "|8          |timmins, ontario, canada          |\n",
      "|9          |germantown, tennessee, usa        |\n",
      "|10         |albacete, wisconsin, spain        |\n",
      "+-----------+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " spark.sql(\"SELECT * FROM users\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------\n",
      " ISBN        | 0195153448                                                                                         \n",
      " Book-Title  | Classical Mythology                                                                                \n",
      " Book-Author | Mark P. O. Morford                                                                                 \n",
      " Publisher   | Oxford University Press                                                                            \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------\n",
      " ISBN        | 0002005018                                                                                         \n",
      " Book-Title  | Clara Callan                                                                                       \n",
      " Book-Author | Richard Bruce Wright                                                                               \n",
      " Publisher   | HarperFlamingo Canada                                                                              \n",
      "-RECORD 2---------------------------------------------------------------------------------------------------------\n",
      " ISBN        | 0060973129                                                                                         \n",
      " Book-Title  | Decision in Normandy                                                                               \n",
      " Book-Author | Carlo D'Este                                                                                       \n",
      " Publisher   | HarperPerennial                                                                                    \n",
      "-RECORD 3---------------------------------------------------------------------------------------------------------\n",
      " ISBN        | 0374157065                                                                                         \n",
      " Book-Title  | Flu: The Story of the Great Influenza Pandemic of 1918 and the Search for the Virus That Caused It \n",
      " Book-Author | Gina Bari Kolata                                                                                   \n",
      " Publisher   | Farrar Straus Giroux                                                                               \n",
      "-RECORD 4---------------------------------------------------------------------------------------------------------\n",
      " ISBN        | 0393045218                                                                                         \n",
      " Book-Title  | The Mummies of Urumchi                                                                             \n",
      " Book-Author | E. J. W. Barber                                                                                    \n",
      " Publisher   | W. W. Norton &amp; Company                                                                         \n",
      "-RECORD 5---------------------------------------------------------------------------------------------------------\n",
      " ISBN        | 0399135782                                                                                         \n",
      " Book-Title  | The Kitchen God's Wife                                                                             \n",
      " Book-Author | Amy Tan                                                                                            \n",
      " Publisher   | Putnam Pub Group                                                                                   \n",
      "-RECORD 6---------------------------------------------------------------------------------------------------------\n",
      " ISBN        | 0425176428                                                                                         \n",
      " Book-Title  | What If?: The World's Foremost Military Historians Imagine What Might Have Been                    \n",
      " Book-Author | Robert Cowley                                                                                      \n",
      " Publisher   | Berkley Publishing Group                                                                           \n",
      "-RECORD 7---------------------------------------------------------------------------------------------------------\n",
      " ISBN        | 0671870432                                                                                         \n",
      " Book-Title  | PLEADING GUILTY                                                                                    \n",
      " Book-Author | Scott Turow                                                                                        \n",
      " Publisher   | Audioworks                                                                                         \n",
      "-RECORD 8---------------------------------------------------------------------------------------------------------\n",
      " ISBN        | 0679425608                                                                                         \n",
      " Book-Title  | Under the Black Flag: The Romance and the Reality of Life Among the Pirates                        \n",
      " Book-Author | David Cordingly                                                                                    \n",
      " Publisher   | Random House                                                                                       \n",
      "-RECORD 9---------------------------------------------------------------------------------------------------------\n",
      " ISBN        | 074322678X                                                                                         \n",
      " Book-Title  | Where You'll Find Me: And Other Stories                                                            \n",
      " Book-Author | Ann Beattie                                                                                        \n",
      " Publisher   | Scribner                                                                                           \n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " spark.sql(\"SELECT * FROM books\").show(10, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recommended books for user 0 are: \n"
     ]
    }
   ],
   "source": [
    " print(\"The recommended books for user \" + str(user) + \" are: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave it as exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The users who might be interested on the book 0 are: \n"
     ]
    }
   ],
   "source": [
    " print(\"The users who might be interested on the book \" + str(book) + \" are: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave it as exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Additional exercise\n",
    "\n",
    "Given the current status of this notebook, redo its content such that major tasks are split into \n",
    "various notebooks, or Python modules. \n",
    "The purpose is to modularize code having in mind the setup of a real recommender system. That is:\n",
    "- A downloader module, with focus on downloading data, cleasing it, and then storing it in a data store.\n",
    "- A recommender building module, to create a recommendation model\n",
    "- A recommender running module, to pre-compute recommendations and to save them in a data store.\n",
    "- A recommender server, to retrieve recommendations upon queries made to the data store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Learning Spark - Lightning-Fast Data Analytics, 2nd Ed. J. Damji, B. Wenig, T. Das, and D. Lee. O'Reilly, 2020\n",
    "* Spark: The Definitive Guide - Big Data Processing Made Simple, 1st Ed. B. Chambers and M. Zaharia. O'Reilly, 2018\n",
    "* http://spark.apache.org/docs/latest/ml-guide.html\n",
    "* https://docs.python.org/3/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "249px",
    "width": "332px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.98px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
